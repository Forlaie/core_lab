[
    {
        "series": "ACE '25",
        "location": "",
        "keywords": "Testing, CS2, Programming, Gamification, Computing Education",
        "numpages": "10",
        "pages": "46\u201355",
        "booktitle": "Proceedings of the 27th Australasian Computing Education Conference",
        "abstract": "Students struggle to understand why rigorous testing is necessary, often testing with a small number of examples and testing interactively instead of through a framework. Our goal is to encourage students to meaningfully engage in the testing process. We do so by developing a system, Codetierlist, that gamifies the process of testing on a programming assignment in a first-year programming course (CS2). Student tests for a programming assignment are run against both the instructor solution and other student solutions, and students receive feedback, in the form of a tier-based ranking, on how well their solution compares to fellow students within the shared student test suite. We compared the tests and assignment solutions students produced with and without Codetierlist. We also gathered student and instructor feedback on the experience of using the tool and measured student motivation and self-efficacy regarding testing. Students wrote more functionally correct code with Codetierlist, and they wrote significantly more and more precise tests with Codetierlist, even identifying previously unknown bugs in the instructor solution. We did not detect any changes to student self-efficacy, but students reported feeling more positive about testing and more motivated to test with Codetierlist. However, we also detected negative effects from the gamification method selected, as some students whose code was placed in a lower tier felt discouraged and less able to succeed. Additionally, we found that improvements to motivation and efficacy may vary based on a student\u2019s prior experience. We provide the community with a tool, Codetierlist, for motivating students to engage more actively with testing in an assignment setting, and identify positive aspects of providing students with a target to test against. However, we reiterate the need for caution when introducing gamified elements that might be viewed as encouraging competition, as students who receive negative feedback from a comparison may feel unable to improve their situation.",
        "doi": "10.1145/3716640.3716646",
        "url": "https://doi.org/10.1145/3716640.3716646",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400714252",
        "year": "2025",
        "title": "Codetierlist: Competitive Gamification's Impact on Self-Efficacy, Motivation, and Performance in Computing Education",
        "author": [
            "Yousef Bulbulia",
            "Ido Ben Haim",
            "Jackson Lee",
            "Brian Zhang",
            "Daksh Malhotra",
            "Andrew Petersen",
            "Michael Liut"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "02Bulbulia25"
    },
    {
        "series": "ITiCSE 2024",
        "location": "Milan, Italy",
        "keywords": "curriculum, data engineering, data systems, database, education, industry, knowledge gap, skill set",
        "numpages": "29",
        "pages": "95\u2013123",
        "booktitle": "2024 Working Group Reports on Innovation and Technology in Computer Science Education",
        "abstract": "Data systems have been an important part of computing curricula for decades, and an integral part of data-focused industry roles such as software developers, data engineers, and data scientists. However, the field of data systems encompasses a large number of topics ranging from data manipulation and database distribution to creating data pipelines and data analytics solutions. Due to the slow nature of curriculum development, it remains unclear (i) which data systems topics are recommended across diverse higher education curriculum guidelines, (ii) which topics are taught in higher education data systems courses, and (iii) which data systems topics are actually valued in data-focused industry roles. In this study, we analyzed computing curriculum guidelines, course contents, and industry needs regarding data systems to uncover discrepancies between them. Our results show, for example, that topics such as data visualization, data warehousing, and semi-structured data models are valued in industry, yet seldom taught in courses. This work allows professionals to further align curriculum guidelines, higher education, and data systems industry to better prepare students for their working life by focusing on relevant skills in data systems education.",
        "doi": "10.1145/3689187.3709609",
        "url": "https://doi.org/10.1145/3689187.3709609",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400712081",
        "year": "2025",
        "title": "Data Systems Education: Curriculum Recommendations, Course Syllabi, and Industry Needs",
        "author": [
            "Daphne Miedema",
            "Toni Taipalus",
            "Vangel V. Ajanovski",
            "Abdussalam Alawini",
            "Martin Goodfellow",
            "Michael Liut",
            "Svetlana Peltsverger",
            "Tiffany Young"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "01Miedema25"
    },
    {
        "series": "Koli Calling '24",
        "location": "",
        "keywords": "Visualization, Prior Programming Experience, Sketching, Mental Models, Computing Education",
        "numpages": "2",
        "articleno": "34",
        "booktitle": "Proceedings of the 24th Koli Calling International Conference on Computing Education Research",
        "abstract": "This research explores the challenges novice students face in un- derstanding and tracing programs, particularly when lacking prior programming experience. Novices often struggle with ineffective tracing strategies and developing accurate mental models, which are crucial for advancing in programming. While visualization and sketching have been proposed as methods to improve these skills, their effectiveness remains mixed. In my Ph.D. I propose studying investigating the potential of interactive visualizations and sketching, grounded in multiple representations theory, to enhance learner engagement, reduce cognitive load, and support various levels of program comprehension. By examining how these tools can be integrated into programming education, particularly through formats that range from analogies to concrete code representations, this work aims to offer practical guidelines to make programming more accessible and intuitive, thereby addressing a critical gap in computing education",
        "doi": "10.1145/3699538.3699572",
        "url": "https://doi-org.myaccess.library.utoronto.ca/10.1145/3699538.3699572",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400710384",
        "year": "2024",
        "title": "Enhancing Novice Programmers? Understanding through Interactive Visualizations and Multiple Representations",
        "author": [
            "Naaz Sibia"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "10.1145/3699538.3699572"
    },
    {
        "series": "ITiCSE 2024",
        "location": "Milan, Italy",
        "keywords": "computer science education, curriculum, technical writing skills, wac, wid, written communication, wtl",
        "numpages": "7",
        "pages": "332\u2013338",
        "booktitle": "Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1",
        "abstract": "Faculty and industry practitioners recognize written communication to be important in computer science, but it can be challenging to convince students of the same. As student perceptions are molded early in a program of study, we focus on early-year CS students to understand their perceptions towards the importance of writing in CS, with the goal of framing discipline-specific writing pedagogy. We qualitatively analyze responses from first and second-year CS students in a survey about the role of writing in their field. The responses reveal that a majority view writing as an indispensable skill. Specifically, students recognize it as a fundamental skill, applicable across diverse contexts, and uniquely relevant in CS compared to other fields. We identified 4 perceptions that they hold which are helpful to their development as writers: that writing is a useful fundamental skill, which is useful for achieving various goals in a variety of contexts, and that writing in CS is different than in other fields. However, 20\\% of responses include reasons why writing is not important in CS, and we identify 4 perceptions harmful to students' development as writers: that writing skills can be avoided, are defined narrowly, do not need to be developed beyond a baseline, and come at the cost of computing skills. We believe that there is an opportunity to align discipline-specific writing instruction with these useful and harmful perceptions.",
        "doi": "10.1145/3649217.3653576",
        "url": "https://doi-org.myaccess.library.utoronto.ca/10.1145/3649217.3653576",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400706004",
        "year": "2024",
        "title": "Early Computer Science Students' Perspectives Towards The Importance Of Writing",
        "author": [
            "Rutwa Engineer",
            "Naaz Sibia",
            "Michael Kaler",
            "Bogdan Simion",
            "Lisa Zhang"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "10.1145/3649217.3653576"
    },
    {
        "series": "Koli Calling '24",
        "location": "",
        "keywords": "visualization, multiple representations theory, code tracing, introductory programming",
        "numpages": "2",
        "articleno": "25",
        "booktitle": "Proceedings of the 24th Koli Calling International Conference on Computing Education Research",
        "abstract": "This pilot study explores how visualization strategies, grounded in multiple representations theory, impact novice students\u2019 engagement, and cognitive load during program tracing tasks. Students were were shown a visualization of the three-variable swap problem at the start of an introductory programming course (CS1) at a large public North American research-intensive university. We compared three conditions: interactive multiple representations, Python Tutor (a single-representation tool), and text-only methods. Preliminary results indicate that interactive multiple representations increase engagement for students with prior programming experience, while no significant differences were observed for students without prior experience. These findings suggest that while multiple representations may boost engagement, identifying how to effectively support students of all experience levels and reduce cognitive load requires further study.",
        "doi": "10.1145/3699538.3699587",
        "url": "https://doi.org/10.1145/3699538.3699587",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400710384",
        "year": "2024",
        "title": "Exploring the Impact of Multiple Representations in Introductory Programming: A Pilot Study",
        "author": [
            "Naaz Sibia",
            "Valeria Ramirez Osorio",
            "Angela Zavaleta Bernuy",
            "Efthimia Aivaloglou",
            "Rutwa Engineer",
            "Andrew Petersen",
            "Michael Liut",
            "Carolina Nobre"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "11Sibia24"
    },
    {
        "keywords": "artificial intelligence in education, collaborative learning with ai, human-ai collaboration, large language models, transparency, tutoring systems",
        "numpages": "30",
        "articleno": "499",
        "month": "November",
        "journal": "Proc. ACM Hum.-Comput. Interact.",
        "abstract": "Personalized chatbot-based teaching assistants can be crucial in addressing increasing classroom sizes, especially where direct teacher presence is limited. Large language models (LLMs) offer a promising avenue, with increasing research exploring their educational utility. However, the challenge lies not only in establishing the efficacy of LLMs but also in discerning the nuances of interaction between learners and these models, which impact learners' engagement and results. We conducted a formative study in an undergraduate computer science classroom (N=145) and a controlled experiment on Prolific (N=356) to explore the impact of four pedagogically informed guidance strategies on the learners' performance, confidence and trust in LLMs. Direct LLM answers marginally improved performance, while refining student solutions fostered trust. Structured guidance reduced random queries as well as instances of students copy-pasting assignment questions to the LLM. Our work highlights the role that teachers can play in shaping LLM-supported learning environments.",
        "doi": "10.1145/3687038",
        "url": "https://doi.org/10.1145/3687038",
        "number": "CSCW2",
        "volume": "8",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "issue_date": "November 2024",
        "year": "2024",
        "title": "Guiding Students in Using LLMs in Supported Learning Environments: Effects on Interaction Dynamics, Learner Performance, Confidence, and Trust",
        "author": [
            "Harsh Kumar",
            "Ilya Musabirov",
            "Mohi Reza",
            "Jiakai Shi",
            "Xinyuan Wang",
            "Joseph Jay Williams",
            "Anastasia Kuzminykh",
            "Michael Liut"
        ],
        "ENTRYTYPE": "article",
        "ID": "11Kumar24"
    },
    {
        "pages": "23-32",
        "month": "Oct.",
        "year": "2024",
        "author": [
            "Jessica Y. Bo",
            "Harsh Kumar",
            "Michael Liut",
            "Ashton Anderson"
        ],
        "journal": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing",
        "number": "1",
        "abstract": "Large Language Models (LLMs) are increasingly being used in educational settings to assist students with assignments and learning new concepts. For LLMs to be effective learning aids, students must develop an appropriate level of trust and reliance on these tools. Misaligned trust and reliance can lead to suboptimal learning outcomes and reduced LLM engagement. Despite their growing presence, there is a limited understanding of achieving optimal transparency and reliance calibration in the educational use of LLMs. In a 3x2 between-subjects experiment conducted in a university classroom setting, we tested the effect of two transparency disclosures (System Prompt and Goal Summary) and an in-conversation Reliability Disclaimer on a GPT-4-based chatbot tutor provided to students for an assignment. Our findings suggest that disclaimer messages included in the responses may effectively mitigate learners\u2019 overreliance on the LLM Tutor in the presence of incorrect advice. Disclosing System Prompt seemed to calibrate students\u2019 confidence in their answers and reduce the occurrence of copy-pasting the exact assignment question to the LLM tutor. Student feedback indicated that they would like transparency framed in terms of performance-based metrics. Our work provides empirical insights on the design of transparency and reliability mechanisms for using LLMs in classrooms.",
        "doi": "10.1609/hcomp.v12i1.31597",
        "url": "https://ojs.aaai.org/index.php/HCOMP/article/view/31597",
        "volume": "12",
        "title": "Disclosures &amp; Disclaimers: Investigating the Impact of Transparency Disclosures and Reliability Disclaimers on Learner-LLM Interactions",
        "ENTRYTYPE": "article",
        "ID": "10Bo24"
    },
    {
        "series": "L@S '24",
        "location": "Atlanta, GA, USA",
        "keywords": "field experiments, human-ai collaboration, large language models, learning engineering, self-reflection",
        "numpages": "12",
        "pages": "86\u201397",
        "booktitle": "Proceedings of the Eleventh ACM Conference on Learning @ Scale",
        "abstract": "Self-reflection on learning experiences constitutes a fundamental cognitive process, essential for consolidating knowledge and enhancing learning efficacy. However, traditional methods to facilitate reflection often face challenges in personalization, immediacy of feedback, engagement, and scalability. Integration of Large Language Models (LLMs) into the reflection process could mitigate these limitations. In this paper, we conducted two randomized field experiments in undergraduate computer science courses to investigate the potential of LLMs to help students engage in post-lesson reflection. In the first experiment (N=145), students completed a take-home assignment with the support of an LLM assistant; half of these students were then provided access to an LLM designed to facilitate self-reflection. The results indicated that the students assigned to LLM-guided reflection reported somewhat increased self-confidence compared to peers in a no-reflection control and a non-significant trend towards higher scores on a later assessment. Thematic analysis of students' interactions with the LLM showed that the LLM often affirmed the student's understanding, expanded on the student's reflection, and prompted additional reflection; these behaviors suggest ways LLM-interaction might facilitate reflection. In the second experiment (N=112), we evaluated the impact of LLM-guided self-reflection against other scalable reflection methods, such as questionnaire-based activities and review of key lecture slides, after assignment. Our findings suggest that the students in the questionnaire and LLM-based reflection groups performed equally well and better than those who were only exposed to lecture slides, according to their scores on a proctored exam two weeks later on the same subject matter. These results underscore the utility of LLM-guided reflection and questionnaire-based activities in improving learning outcomes. Our work highlights that focusing solely on the accuracy of LLMs can overlook their potential to enhance metacognitive skills through practices such as self-reflection. We discuss the implications of our research for the learning-at-scale community, highlighting the potential of LLMs to enhance learning experiences through personalized, engaging, and scalable reflection practices.",
        "doi": "10.1145/3657604.3662042",
        "url": "https://doi.org/10.1145/3657604.3662042",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400706332",
        "year": "2024",
        "title": "Supporting Self-Reflection at Scale with Large Language Models: Insights from Randomized Field Experiments in Classrooms",
        "author": [
            "Harsh Kumar",
            "Ruiwei Xiao",
            "Benjamin Lawson",
            "Ilya Musabirov",
            "Jiakai Shi",
            "Xinyuan Wang",
            "Huayin Luo",
            "Joseph Jay Williams",
            "Anna N. Rafferty",
            "John Stamper",
            "Michael Liut"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "07Kumar24"
    },
    {
        "doi": "10.1109/COMPSAC61105.2024.00027",
        "keywords": "Measurement;Accuracy;Plagiarism;Large language models;Education;Detectors;Chatbots;Large Language Models;ChatGPT;GPT;AI Detectors;Plagiarism;Academic Integrity",
        "url": "https://doi.org/10.1109/COMPSAC61105.2024.00027",
        "pages": "121-126",
        "number": "",
        "volume": "",
        "year": "2024",
        "title": "Detecting LLM-Generated Text in Computing Education: Comparative Study for ChatGPT Cases",
        "booktitle": "2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)",
        "author": [
            "Michael Sheinman Orenstrakh",
            "Oscar Karnalim",
            "Carlos An\u00edbal Su\u00e1rez",
            "Michael Liut"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "06Sheinman24"
    },
    {
        "series": "WCCCE '24",
        "location": "<conf-loc>, <city>Kelowna</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>",
        "keywords": "Thompson sampling, adaptive experimentation, continuous improvement, multiarmed bandit",
        "numpages": "7",
        "articleno": "4",
        "booktitle": "The 26th Western Canadian Conference on Computing Education",
        "abstract": "Randomized A/B comparisons of alternative pedagogical strategies or other course improvements could provide useful empirical evidence for instructor decision-making. However, traditional experiments do not provide a straightforward pathway to rapidly utilize data, increasing the chances that students in an experiment experience the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might aid continuous course improvement. In adaptive experiments, data is analyzed and utilized as different conditions are deployed to students. This can be achieved using machine learning algorithms to identify which actions are more beneficial in improving students\u2019 learning experiences and outcomes. These algorithms can then dynamically deploy the most effective conditions in subsequent interactions with students, resulting in better support for students\u2019 needs. We illustrate this approach with a case study that provides a side-by-side comparison of traditional and adaptive experiments on adding self-explanation prompts in online homework problems in a CS1 course. This work paves the way for exploring the importance of adaptive experiments in bridging research and practice to achieve continuous improvement in educational settings.",
        "doi": "10.1145/3660650.3660659",
        "url": "https://doi.org/10.1145/3660650.3660659",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400709975",
        "year": "2024",
        "title": "Opportunities for Adaptive Experiments to Enable Continuous Improvement in Computer Science Education",
        "author": [
            "Ilya Musabirov",
            "Angela Zavaleta Bernuy",
            "Pan Chen",
            "Michael Liut",
            "Joseph Williams"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "05Musabirov24"
    },
    {
        "series": "WCCCE '24",
        "location": "<conf-loc>, <city>Kelowna</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>",
        "keywords": "CS1, student experience, transition, upper-year",
        "numpages": "7",
        "articleno": "5",
        "booktitle": "The 26th Western Canadian Conference on Computing Education",
        "abstract": "While the challenges experienced by first-year computing students have been well studied, little work has explored the transitions in disciplinary participation and challenges experienced by upper-years. This study explores how students\u2019 needs and challenges evolve through a computing degree. We collected the experiences of first to final-year undergraduate computing students through surveys and interviews. We organized these experiences into themes that we compare against previous literature and illustrate with quotes. Upper-year students perceive changes in (a) levels of support and (b) the kinds of challenges they experience as they progress through the program. Second-year students feel pressured by the increasing difficulty of courses. This pressure increases through the third year as students begin to perceive a need to find employment. The experiences of our students suggest the need to better support the middle years of academic programs. Students in the first year are well-supported in their university transition, but students in the middle are often left to find their way as they develop a deeper understanding of their desired place in the field.",
        "doi": "10.1145/3660650.3660661",
        "url": "https://doi.org/10.1145/3660650.3660661",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400709975",
        "year": "2024",
        "title": "Student Transitions Through an Entire Computing Program",
        "author": [
            "Angela Zavaleta Bernuy",
            "Andrew Chung",
            "Alana Hodge",
            "Ayesha Tayyiba",
            "Michael Liut",
            "Andrew Petersen"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "05Zavaleta24"
    },
    {
        "series": "CHI '24",
        "location": "<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>",
        "keywords": "datasets, gaze detection, neural networks, text tagging",
        "numpages": "18",
        "articleno": "1042",
        "booktitle": "Proceedings of the CHI Conference on Human Factors in Computing Systems",
        "abstract": "Exploring alternative ideas by rewriting text is integral to the writing process. State-of-the-art Large Language Models (LLMs) can simplify writing variation generation. However, current interfaces pose challenges for simultaneous consideration of multiple variations: creating new variations without overwriting text can be difficult, and pasting them sequentially can clutter documents, increasing workload and disrupting writers' flow. To tackle this, we present ABScribe, an interface that supports rapid, yet visually structured, exploration and organization of writing variations in human-AI co-writing tasks. With ABScribe, users can swiftly modify variations using LLM prompts, which are auto-converted into reusable buttons. Variations are stored adjacently within text fields for rapid in-place comparisons using mouse-over interactions on a popup toolbar. Our user study with 12 writers shows that ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances user perceptions of the revision process (d = 2.41, p < 0.001) compared to a popular baseline workflow, and provides insights into how writers explore variations using LLMs.",
        "doi": "10.1145/3613904.3641899",
        "url": "https://doi.org/10.1145/3613904.3641899",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400703300",
        "year": "2024",
        "title": "ABScribe: Rapid Exploration \\& Organization of Multiple Writing Variations in Human-AI Co-Writing Tasks using Large Language Models",
        "author": [
            "Mohi Reza",
            "Nathan M Laundry",
            "Ilya Musabirov",
            "Peter Dushniku",
            "Zhi Yuan \"Michael\" Yu",
            "Kashish Mittal",
            "Tovi Grossman",
            "Michael Liut",
            "Anastasia Kuzminykh",
            "Joseph Jay Williams"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "05Reza24"
    },
    {
        "series": "CHI '24",
        "location": "<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>",
        "keywords": "ChatGPT, Education, GPT-4, Large Language Models, Personalized Reflections, Procrastination",
        "numpages": "18",
        "articleno": "15",
        "booktitle": "Proceedings of the CHI Conference on Human Factors in Computing Systems",
        "abstract": "Traditional interventions for academic procrastination often fail to capture the nuanced, individual-specific factors that underlie them. Large language models (LLMs) hold immense potential for addressing this gap by permitting open-ended inputs, including the ability to customize interventions to individuals' unique needs. However, user expectations and potential limitations of LLMs in this context remain underexplored. To address this, we conducted interviews and focus group discussions with 15 university students and 6 experts, during which a technology probe for generating personalized advice for managing procrastination was presented. Our results highlight the necessity for LLMs to provide structured, deadline-oriented steps and enhanced user support mechanisms. Additionally, our results surface the need for an adaptive approach to questioning based on factors like busyness. These findings offer crucial design implications for the development of LLM-based tools for managing procrastination while cautioning the use of LLMs for therapeutic guidance.",
        "doi": "10.1145/3613904.3642081",
        "url": "https://doi.org/10.1145/3613904.3642081",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400703300",
        "year": "2024",
        "title": "Understanding the Role of Large Language Models in Personalizing and Scaffolding Strategies to Combat Academic Procrastination",
        "author": [
            "Ananya Bhattacharjee",
            "Yuchen Zeng",
            "Sarah Yi Xu",
            "Dana Kulzhabayeva",
            "Minyi Ma",
            "Rachel Kornfield",
            "Syed Ishtiaque Ahmed",
            "Alex Mariakakis",
            "Mary P Czerwinski",
            "Anastasia Kuzminykh",
            "Michael Liut",
            "Joseph Jay Williams"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "05Liut24"
    },
    {
        "series": "SIGCSE 2024",
        "location": "Portland, OR, USA",
        "keywords": "computing education, cs for all, equity, guidelines, high-quality, post-secondary, primary, research, secondary",
        "numpages": "2",
        "pages": "1871\u20131872",
        "booktitle": "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2",
        "abstract": "While there are multiple standards bodies that define characteristics of high-quality, there are limited guidelines on conducting equity-enabling research, particularly in the context of high quality and in computing education. As part of an ACM ITiCSE Working Group in 2023, we engaged in a concept analysis and structured literature review to identify high-impact practices for conducting both high-quality and equity-enabling education research. As a result of this work, we produced a set of guidelines across each major phase of research that integrates characteristics of high-quality education research with those that are necessary for producing research that is designed to honor and meet the needs of various subgroups of learners. Special emphasis is given to the role that the researcher plays in shaping the research based upon how the researcher's lived experiences, perspectives, and training influences their work. During this special session, we will review each set of guidelines and engage attendees in reflection and discussion of them and how they can use the guidelines to enhance their education research.",
        "doi": "10.1145/3626253.3633402",
        "url": "https://doi.org/10.1145/3626253.3633402",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400704246",
        "year": "2024",
        "title": "Unlocking Excellence in Educational Research: Guidelines for High-Quality Research that Promotes Learning for All",
        "author": [
            "Monica M. McGill",
            "Sarah Heckman",
            "Michael Liut",
            "Ismaila Temitayo Sanusi",
            "Claudia Szabo"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03McGill24"
    },
    {
        "series": "SIGCSE 2024",
        "location": "Portland, OR, USA",
        "keywords": "help-seeking behavior, intelligent tutoring systems, on-demand hints, self-regulated learning",
        "numpages": "2",
        "pages": "1578\u20131579",
        "booktitle": "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 2",
        "abstract": "Asking for help (help-seeking) is a recognized and effective problem-solving strategy. This study investigates students' interaction with on-demand hints (automated hints requested by students) and assesses their impact on learning progress. We conducted an A/B experiment in a third-year computer science database course, offering hints for selected SQL problems with different hint designs. We collected data on students' code submissions, grades, and hint requests, and we administered a survey to gather feedback and gauge student perception of the hints. Many students accessed hints immediately without attempting the problem first, often requesting multiple hints in quick succession. While students perceived the hints to be valuable, we did not detect an impact on student problem-solving. These insights could inform future studies on the possible impact of students' attitudes toward hints, and how different types of hints might impact uptake and perception of hints.",
        "doi": "10.1145/3626253.3635563",
        "url": "https://doi.org/10.1145/3626253.3635563",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400704246",
        "year": "2024",
        "title": "Do Hints Enhance Learning in Programming Exercises? Exploring Students' Problem-Solving and Interactions",
        "author": [
            "Giang Bui",
            "Nicholas Susanto",
            "Naaz Sibia",
            "Angela Zavaleta Bernuy",
            "Michael Liut",
            "Andrew Petersen"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03Bui24"
    },
    {
        "series": "SIGCSE 2024",
        "location": "<conf-loc>, <city>Portland</city>, <state>OR</state>, <country>USA</country>, </conf-loc>",
        "keywords": "academic dishonesty, academic integrity, assessment, cheating, computer science students, computing students, education",
        "numpages": "7",
        "pages": "757\u2013763",
        "booktitle": "Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1",
        "abstract": "In contrast with studies that have identified why students commit academic offences, many educators are familiar with the excuse that an accused student did not know the behavior counted as dishonest. Given the variations in policy and the ways collaboration and code sharing occur in professional and hobbyist spaces, this might be plausible. Mismatches between students' conceptions of academic honesty and course policy can have major consequences, from being kicked out of programs to being too nervous to study with peers. In this work, we investigate what students understand about academic integrity in computer science courses and if there are differences based on university, country, demographic, or online versus in-person courses. We present a study that surveys undergraduate computer science students (N = 1,011) at three universities (Australia, Canada, and the United States of America). The results show that all three institutions take academic integrity seriously, and their students are aware of its importance, but confusion on what is covered under the policies is common. Interestingly, the results also show that course instructors play a huge role as to what students perceive to be a violation of the academic integrity policy at their institution. By understanding student's perspectives on academic integrity, educators can better develop policies and practices that reduce inadvertent and mistaken violations of academic integrity policies.",
        "doi": "10.1145/3626252.3630753",
        "url": "https://doi.org/10.1145/3626252.3630753",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400704239",
        "year": "2024",
        "title": "\"I Didn't Know\": Examining Student Understanding of Academic Dishonesty in Computer Science",
        "author": [
            "Michael Liut",
            "Anna Ly",
            "Jessica Jia-Ni Xu",
            "Justice Banson",
            "Paul Vrbik",
            "Caroline D. Hardin"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03Liut24"
    },
    {
        "series": "Koli Calling '23",
        "location": "<conf-loc>, <city>Koli</city>, <country>Finland</country>, </conf-loc>",
        "keywords": "Data Systems Education, Databases, Misconceptions, SQL",
        "numpages": "12",
        "articleno": "10",
        "booktitle": "Proceedings of the 23rd Koli Calling International Conference on Computing Education Research",
        "abstract": "In recent years, database education has been receiving more attention, with research in various directions such as the development of tools for education, the analysis of students\u2019 homework, and the exploration of misconceptions. Misconceptions are mistakes in student reasoning that lead to errors during problem-solving. Recent work has documented misconceptions and errors in SQL. In this study we test the prevalence of several of these misconceptions through a multiple-choice questionnaire, to see if they hold on a larger, more diverse, student population. We found that all misconceptions are held to some extent, with prevalence scores ranging from one to fifty-two percent of the student population. Additionally, we have uncovered previously unidentified areas of struggle, allowing us to identify new misconceptions.",
        "doi": "10.1145/3631802.3631821",
        "url": "https://doi.org/10.1145/3631802.3631821",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400716539",
        "year": "2024",
        "title": "\u201cThere is no ambiguity on what to return\u201d: Investigating the Prevalence of SQL Misconceptions",
        "author": [
            "Daphne Miedema",
            "Michael Liut",
            "George H. L. Fletcher",
            "Efthimia Aivaloglou"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "01Miedema23"
    },
    {
        "series": "ITiCSE-WGR '23",
        "location": "Turku, Finland",
        "keywords": "computer science education research, computing education, equity, evidence, high quality, k-12, post-secondary, primary, research, secondary, standards, tertiary",
        "numpages": "27",
        "pages": "30\u201356",
        "booktitle": "Proceedings of the 2023 Working Group Reports on Innovation and Technology in Computer Science Education",
        "abstract": "Problem. To investigate and identify promising practices in equitable K-12 and tertiary computer science (CS) education, the capacity for education researchers to conduct this research must be rapidly built globally. Simultaneously, concerns have arisen over the last few years about the quality of research that is being conducted and the lack of research that supports teaching all students computing.Research Question. Our research question for our study was: In what ways can existing research standards and practices inform methodologically sound, equity-enabling computing education research?Methodology. We conducted a concept analysis using existing research and various standards (e.g. European Educational Research Association, Australian Education Research Organisation, American Psychological Association). We then synthesised key features in the context of equity-focused K-12 computing education research.Findings. We present a set of guidelines for general research design that takes into account best practices across the standards that are infused with equity-enabling research practices.Implications. Our guidelines will directly impact future equitable computing education research by providing guidance on conducting high-quality research such that the findings can be aggregated and impact future policy with evidence-based results. Because we have crafted these guidelines to be broadly applicable across a variety of settings, we believe that they will be useful to researchers operating in a variety of contexts.",
        "doi": "10.1145/3623762.3633495",
        "url": "https://doi.org/10.1145/3623762.3633495",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400704055",
        "year": "2023",
        "title": "Conducting Sound, Equity-Enabling Computing Education Research",
        "author": [
            "Monica M. McGill",
            "Sarah Heckman",
            "Christos Chytas",
            "Michael Liut",
            "Vera Kazakova",
            "Ismaila Temitayo Sanusi",
            "Selina Marianna Shah",
            "Claudia Szabo"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "12McGill23"
    },
    {
        "series": "ICER '23",
        "location": "Chicago, IL, USA",
        "numpages": "2",
        "pages": "16\u201317",
        "booktitle": "Proceedings of the 2023 ACM Conference on International Computing Education Research - Volume 2",
        "doi": "10.1145/3568812.3603471",
        "url": "https://doi.org/10.1145/3568812.3603471",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450399753",
        "year": "2023",
        "title": "MSMI1: Towards a Validated SQL Misconceptions Instrument",
        "author": [
            "Daphne Miedema",
            "Michael Liut",
            "George Fletcher",
            "Efthimia Aivaloglou"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "09Miedema23"
    },
    {
        "series": "ITiCSE 2023",
        "location": "Turku, Finland",
        "keywords": "educational technology, voice recording, voice submission",
        "numpages": "2",
        "pages": "585\u2013586",
        "booktitle": "Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2",
        "abstract": "Generating self-explanations has been identified as a successful strategy in helping learners engage with course content and organize what they learn in a structured format. While typing an explanation may allow more structure and formality, explaining by voice can be more natural and help free cognitive resources to focus on learning goals and understanding concepts. As we investigated the effects and students' perceptions of using voice or text to self-explain new course concepts, we failed to find a tool that would meet our needs. We present our work in designing and developing VoiceEx, a submission courseware that allows text and voice input to collect data in both mediums. VoiceEx was created to support a self-explanations intervention for computer science students; however, given its features and the advantages of being able to collect spoken responses, it can be used in a variety of environments. Future refinement of this tool includes artificial intelligence features to better guide students' submissions.",
        "doi": "10.1145/3587103.3595284",
        "url": "https://doi.org/10.1145/3587103.3595284",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400701399",
        "year": "2023",
        "title": "VoiceEx: Voice Submission System for Interventions in Education",
        "author": [
            "Angela Zavaleta Bernuy",
            "Naaz Sibia",
            "Pan Chen",
            "Chloe Huang",
            "Andrew Petersen",
            "Joseph Jay Williams",
            "Michael Liut"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "06Bernuy23"
    },
    {
        "series": "ITiCSE 2023",
        "location": "Turku, Finland",
        "keywords": "educational technology, reflection prompts, self-explanations, voice recording",
        "numpages": "1",
        "pages": "641",
        "booktitle": "Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 2",
        "abstract": "In this poster, we present a pilot study investigating the impact of the medium used for self-explanation on students' performance outcomes in a databases course. We did not see notable differences in student performance based on the medium they used for self-explanation. We also note that while most students prefer using text to submit self-explanations, their preferences may differ when they use voice.",
        "doi": "10.1145/3587103.3594188",
        "url": "https://doi.org/10.1145/3587103.3594188",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400701399",
        "year": "2023",
        "title": "Self-Explanation Modality: Effects on Student Performance?",
        "author": [
            "Angela Zavaleta Bernuy",
            "Jessica Jia-Ni Xu",
            "Naaz Sibia",
            "Joseph Jay Williams",
            "Andrew Petersen",
            "Michael Liut"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "06Zavaleta23"
    },
    {
        "series": "EduCHI '23",
        "location": "Hamburg, Germany",
        "keywords": "A/B Testing, HCI Education, Iterative Design",
        "numpages": "6",
        "pages": "43\u201348",
        "booktitle": "Proceedings of the 5th Annual Symposium on HCI Education",
        "abstract": "This paper explores the use of A/B testing as a pedagogical tool for iterative design in HCI classrooms and outlines a vision for experiment-inspired design. The traditional focus on the statistical aspects of A/B testing education has meant that the equally crucial role of iterative design embedded within the experimental process has not received commensurate attention. By incorporating iterative design learning activities that are scaffolded by A/B testing tools, HCI students can gain transferable skills and experience in doing multiple cycles of ideation, prototyping, testing, and evaluation, and simultaneously contribute to continual course improvement. We reconsider the role of experimentation in HCI education as a means for exploring complex design spaces. Drawing from our experience in teaching this approach and conducting education research involving sequences of online controlled experiments, we present examples of how to use A/B testing as a pedagogical tool for iterative design in HCI classrooms.",
        "doi": "10.1145/3587399.3587412",
        "url": "https://doi.org/10.1145/3587399.3587412",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9798400707377",
        "year": "2023",
        "title": "Using A/B Testing as a Pedagogical Tool for Iterative Design in HCI Classrooms",
        "author": [
            "Mohi Reza",
            "Ilya Musabirov",
            "Nathan Laundry",
            "Michael Liut",
            "Joseph Jay Williams"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "04Reza23"
    },
    {
        "series": "SIGCSE 2023",
        "location": "Toronto ON, Canada",
        "keywords": "email, open rates, student engagement",
        "numpages": "1",
        "pages": "1363",
        "booktitle": "Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2",
        "abstract": "Instructors often prefer to use email for course communication. The use of emails has been widely discussed in the fields of marketing and behavioural design, but the prevalence of email in education makes it important for instructors to collect metrics on emails to see how students engage with them. One component of emails are the subject lines, which constitute as one of the first things a receiver sees before deciding to open an email. This poster discusses a case study at deploying an email intervention in an online CS1 course. We investigate how the length of subject lines impact the rate at which students open emails of a particular type that prompts them to start their homework early. We aim to share key results to inform instructors how to design their emails to better reach students. Further, we highlight the potential benefits for instructors when collecting and analyzing email engagement data.",
        "doi": "10.1145/3545947.3576307",
        "url": "https://doi.org/10.1145/3545947.3576307",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450394338",
        "year": "2023",
        "title": "Investigating Subject Lines Length on Students' Email Open Rates",
        "author": [
            "Elexandra Tran",
            "Angela Zavaleta Bernuy",
            "Bogdan Simion",
            "Michael Liut",
            "Andrew Petersen",
            "Joseph Jay Williams"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03Tran23"
    },
    {
        "series": "SIGCSE 2023",
        "location": "Toronto ON, Canada",
        "keywords": "adaptive experiments, continuous improvement, field experiments, multiarmed bandits, thompson sampling",
        "numpages": "1",
        "pages": "1279",
        "booktitle": "Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2",
        "abstract": "Drawing inspiration from machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. We discuss an example side-by-side comparison of traditional and adaptive experimentation of self-explanation prompts in online homework problems in a CS1 course. This provides the first step in exploring the future of how this approach can help bridge research and practice in continuous course improvement.",
        "doi": "10.1145/3545947.3576225",
        "url": "https://doi.org/10.1145/3545947.3576225",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450394338",
        "year": "2023",
        "title": "A Case Study in Opportunities for Adaptive Experiments to Enable Rapid Continuous Improvement",
        "author": [
            "Ilya Musabirov",
            "Angela Zavaleta Bernuy",
            "Michael Liut",
            "Joseph Jay Williams"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03Musabirov23"
    },
    {
        "series": "SIGCSE 2023",
        "location": "Toronto ON, Canada",
        "keywords": "adaptive field experiments, computer science education, experimental design, machine learning, multiarmed bandits, statistical analysis",
        "numpages": "1",
        "pages": "1179",
        "booktitle": "Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2",
        "abstract": "Digital experiments can be used in CSedu to test hypotheses about interventions and conditions' efficacy (or inefficacy). This workshop will discuss and deconstruct the design process and analysis for various experiments conducted in CS1. E.g., experiments testing which explanations students find helpful, which emails get them to start homework early, or which webpages effectively encourage and motivate students. This workshop teaches participants how to conduct, interpret, and analyze adaptive field experiments. These adaptive experiments employ machine learning algorithms to analyze experiments during deployment and dynamically shift the allocation of arms/conditions to give future students better conditions more rapidly. Adaptive field experiments can accelerate scientific discovery by enabling more complex experimental designs and increasing statistical power by phasing conditions in and out more efficiently. The workshop is supported by a 5-year NSF grant to build software tools and a digital community, gathering instructors, domain scientists and methodologists to teach them how to run adaptive experiments. The methodological focus includes understanding: (1) which algorithms are best for adaptive experiments that meet domain scientists' needs in specific experimental designs and data sets; (2) which hypothesis tests and Bayesian analyses to choose. Software companies use these innovative methodologies extensively to continuously improve product design. This workshop demonstrates how the same methods can be used in CSedu to improve research rigor and accelerate educational research implementation, ultimately improving student outcomes.",
        "doi": "10.1145/3545947.3569635",
        "url": "https://doi.org/10.1145/3545947.3569635",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450394338",
        "year": "2023",
        "title": "Designing, Deploying, and Analyzing Adaptive Educational Field Experiments",
        "author": [
            "Joseph Jay Williams",
            "Nathan Laundry",
            "Ilya Musabirov",
            "Angela Zavaleta Bernuy",
            "Michael Liut"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03Williams23"
    },
    {
        "series": "SIGCSE 2023",
        "location": "Toronto ON, Canada",
        "keywords": "cs2, prior experience, self-efficacy, confidence, cs1, prediction",
        "numpages": "7",
        "pages": "889\u2013895",
        "booktitle": "Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1",
        "abstract": "Previous work has reported on the advantageous effects of prior experience in CS1, but it remains unclear whether these effects fade over a sequence of introductory programming courses. Furthermore, while student perceptions suggest that prior experience remains important, studies have reported that a student's expectation of their performance is a more accurate predictor of outcome. We aim to confirm if prior experience (formal or informal) provides short-term and long-term advantages in computing courses or if the advantage fades. Furthermore, we explore whether the expectation of performance is a more accurate predictor of student success than informal and formal prior experience. To explore these questions, we deployed surveys in a CS1 course to gauge students' level of prior experience in programming, prediction of final exam grades, and self-efficacy to succeed in university. Grades from CS1 and CS2 were also collected. We observed a persistent (1-letter grade) gap between the performance of students with no prior experience and those with any experience, but we did not observe a noteworthy gap when comparing student performance based on formal or informal experience. We also observed differences in self-efficacy and retention rates between different levels of prior experience. Lastly, we confirm that success in CS1 can be better reflected and predicted by some controllable factors, such as students' perceptions of ability.",
        "doi": "10.1145/3545945.3569752",
        "url": "https://doi.org/10.1145/3545945.3569752",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450394314",
        "year": "2023",
        "title": "Prior Programming Experience: A Persistent Performance Gap in CS1 and CS2",
        "author": [
            "Giang Bui",
            "Naaz Sibia",
            "Angela Zavaleta Bernuy",
            "Michael Liut",
            "Andrew Petersen"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03Bui23"
    },
    {
        "series": "DataEd '22",
        "location": "Philadelphia, PA, USA",
        "keywords": "Active Learning, Long-Term Memory, Computer Science Education, Reflective Prompts, Databases, Student Performance",
        "numpages": "6",
        "pages": "32\u201337",
        "booktitle": "1st International Workshop on Data Systems Education",
        "abstract": "Motivation: Prior literature has identified student reflections as a way to encourage students to express their thoughts in a structured and focused manner. Objectives: Our goal is to examine the impact of reflections in a third year database systems course, which employs an active learning approach and classroom environment. Specifically, we are interested in seeing whether reflecting on key concepts covered in a preparatory component before lecture had an impact on student\u2019s immediate and long-term performance. Methods: Students were divided into two groups, and asked to reflect on different topics after watching lecture videos before completing their homework exercises for 3 weeks. Results: We observed that students who reflected on lecture concepts performed better on homework exercises that covered those same concepts than students who did not reflect on those same concepts. Moreover, students who reflected performed better in subsequent assessments than students who did not reflect at all. Implications: Reflection as a part of the preparatory component in flipped classrooms is a useful component in conceptual understanding. Further research and investigation should be pursued into ways of prompting reflection, and assessing this component in database courses.",
        "doi": "10.1145/3531072.3535323",
        "url": "https://doi.org/10.1145/3531072.3535323",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450393508",
        "year": "2022",
        "title": "The Positive Effects of Using Reflective Prompts in a Database Course",
        "author": [
            "Naaz Sibia",
            "Michael Liut"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "06Liut22"
    },
    {
        "series": "SIGCSE 2022",
        "location": "Providence, RI, USA",
        "keywords": "surveys, open-ended questions, voice responses, cs1",
        "numpages": "1",
        "pages": "1124",
        "booktitle": "Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 2",
        "abstract": "With the widespread usage of mobile devices, users can now choose to provide input through voice or text. As researchers frequently ask students open-ended questions, we want to explore a natural mode to obtain better feedback in surveys. This study details a preliminary study demonstrating the importance of allowing students to choose between voice or text input to respond to surveys. A survey with several open-ended questions was deployed in a CS1 course. Correlations between the gender of the respondent and their method of responding were evaluated. We found that voice responses tended to be longer and preferred more by females relative to male students.",
        "doi": "10.1145/3478432.3499087",
        "url": "https://doi.org/10.1145/3478432.3499087",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450390712",
        "year": "2022",
        "title": "Investigating the Impact of Voice Response Options in Surveys",
        "author": [
            "Pan Chen",
            "Naaz Sibia",
            "Angela Zavaleta Bernuy",
            "Michael Liut",
            "Joseph Jay Williams"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "03Liut22"
    },
    {
        "series": "Koli Calling '21",
        "location": "Joensuu, Finland",
        "keywords": "SQL Automarking, Partial Marking, String Regularities, String Similarity, Database Course Tools, Student Feedback Enhancement",
        "numpages": "3",
        "articleno": "37",
        "booktitle": "21st Koli Calling International Conference on Computing Education Research",
        "abstract": "This work introduces and demonstrates the viability of a novel SQL automarking tool (\u201cSQAM\u201d) that: (1) provides a fair grade to the student, one which matches the student\u2019s effort and understanding of the course material, and (2) to provide personalized feedback, allowing the student to remain engaged in the material and learn from their mistakes while still being in that headspace. Additionally, we strive to ensure that our tool maintains the same standards (grade and feedback) that a highly qualified member of teaching staff would produce, so we compare and contrast our automarker\u2019s results to that of teaching assistants over several historic offerings of the same database course at a large research intensive public institution, while reducing the grading time, thus enabling the teaching staff to channel more time into instruction. Furthermore, we describe SQAM\u2019s design and our model which applies the aggregate result of four different string similarity metrics to compute solution similarity in conjunction with our discretization process to fairly evaluate a student\u2019s submission. Our results show that SQAM produces very similar grades to those which were historically given by teaching assistants. ",
        "doi": "10.1145/3488042.3489970",
        "url": "https://doi.org/10.1145/3488042.3489970",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450384889",
        "year": "2021",
        "title": "Building a Better SQL Automarker for Database Courses",
        "author": [
            "Muyu Wang",
            "Naaz Sibia",
            "Ilir Dema",
            "Michael Liut",
            "Carlos An\u00edbal Su\u00e1rez"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "11Liut20"
    },
    {
        "series": "Koli Calling '21",
        "location": "Joensuu, Finland",
        "keywords": "retention, spatial skills, socioeconomic status, CS1, gender",
        "numpages": "10",
        "articleno": "4",
        "booktitle": "21st Koli Calling International Conference on Computing Education Research",
        "abstract": "Motivation Prior studies have established that training spatial skills may improve outcomes in computing courses. Very few of these studies have, however, explored the impact of spatial skills training on women or examined its relationship with other factors commonly explored in the context of academic performance, such as socioeconomic background and self-efficacy. Objectives In this study, we report on a spatial skills intervention deployed in a computer programming course (CS1) in the first year of a post-secondary program. We explore the relationship between various demographic factors, course performance, and spatial skills ability at both the beginning and end of the term. Methods Data was collected using a combination of demographic surveys, existing self-efficacy and CS1 content instruments, and the Revised PVST:R spatial skills assessment. Spatial skills were evaluated both at the beginning of the term and at the end, after spatial skills training was provided. Results While little evidence was found to link spatial skills to socioeconomic status or self-efficacy, both gender identity and previous experience in computing were found to be correlated to spatial skills ability at the start of the course. Women initially recorded lower spatial skills ability, but after training, the distribution of spatial skills scores for women approached that of men. Discussion These findings suggest that, if offered early enough, spatial skills training may be able to remedy some differences in background that impact performance in computing courses. ",
        "doi": "10.1145/3488042.3488049",
        "url": "https://doi.org/10.1145/3488042.3488049",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450384889",
        "year": "2021",
        "title": "Spatial Skills and Demographic Factors in CS1",
        "author": [
            "Anna Ly",
            "Jack Parkinson",
            "Quintin Cutts",
            "Michael Liut",
            "Andrew Petersen"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "11ly21"
    },
    {
        "numpages": "12",
        "pages": "18\u201329",
        "month": "nov",
        "journal": "ACM Inroads",
        "doi": "10.1145/3494574",
        "url": "https://doi.org/10.1145/3494574",
        "issn": "2153-2184",
        "number": "4",
        "volume": "12",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "issue_date": "December 2021",
        "year": "2021",
        "title": "Practice Report: Six Studies of Spatial Skills Training in Introductory Computer Science",
        "author": [
            "Jack Parkinson",
            "Ryan Bockmon",
            "Quintin Cutts",
            "Michael Liut",
            "Andrew Petersen",
            "Sheryl Sorby"
        ],
        "ENTRYTYPE": "article",
        "ID": "12parkinson21"
    },
    {
        "doi": "10.1145/3450329.3476855",
        "url": "https://doi.org/10.1145/3450329.3476855",
        "year": "2021",
        "pages": "9--14",
        "booktitle": "Proceedings of the 22st Annual Conference on Information Technology Education",
        "author": [
            "Anna Ly",
            "John Edwards",
            "Michael Liut",
            "Andrew Petersen"
        ],
        "title": "Revisiting Syntax Exercises in CS1",
        "ENTRYTYPE": "inproceedings",
        "ID": "10ly2021revisiting"
    },
    {
        "url": "https://cssplice.github.io/LAS20/proc/SPLICE_2020_LS_paper_9.pdf",
        "numpage": "6",
        "year": "2020",
        "booktitle": "Proceedings of the 6th SPLICE Workshop at L@S",
        "title": "Using Discussion Board Data to Hire Teaching Assistants",
        "author": [
            "Arnaud Deza",
            "Haocheng Hu",
            "Vaishvik Maisuria",
            "Michael Liut",
            "Andrew Petersen",
            "Bogdan Simion"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "08Liut20"
    },
    {
        "series": "ITiCSE-WGR '20",
        "location": "Trondheim, Norway",
        "keywords": "collusion, academic integrity, plagiarism, code similarity detection",
        "numpages": "19",
        "pages": "1\u201319",
        "booktitle": "Proceedings of the Working Group Reports on Innovation and Technology in Computer Science Education",
        "doi": "10.1145/3437800.3439201",
        "url": "https://doi.org/10.1145/3437800.3439201",
        "address": "New York, NY, USA",
        "publisher": "Association for Computing Machinery",
        "isbn": "9781450382939",
        "year": "2020",
        "title": "Choosing Code Segments to Exclude from Code Similarity Detection",
        "author": [
            "Simon",
            "Oscar Karnalim",
            "Judy Sheard",
            "Ilir Dema",
            "Amey Karkare",
            "Juho Leinonen",
            "Michael Liut",
            "Renee McCauley"
        ],
        "ENTRYTYPE": "inproceedings",
        "ID": "06Liut20"
    }
]